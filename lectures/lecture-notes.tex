\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{amsmath, amssymb, amsthm, amsfonts, mathtools, bbm, breqn}
\usepackage{enumitem}       % for shortlabels
\usepackage{multicol}       % multiple columns
\usepackage{abraces}        % asymmetric braces
\usepackage{skull}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, arrows, calc, matrix, positioning}

\usepackage{hyperref}
\usepackage[capitalize]{cleveref}


% Citing theorems by name. (source: https://tex.stackexchange.com/questions/109843/cleveref-and-named-theorems)
\makeatletter
\newcommand{\ncref}[1]{\cref{#1}\mynameref{#1}{\csname r@#1\endcsname}}

\def\mynameref#1#2{%
  \begingroup
  \edef\@mytxt{#2}%
  \edef\@mytst{\expandafter\@thirdoffive\@mytxt}%
  \ifx\@mytst\empty\else
  \space(\nameref{#1})\fi
  \endgroup
}
\makeatother

% for the pipe symbol
\usepackage[T1]{fontenc}

\usepackage{xcolor} % Enables a broader range of colors

% Define custom colors
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}

% Note commands
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Blue}{rgb}{0,0,1}
\definecolor{Purple}{rgb}{.75,0,.25}
\newcommand{\rnote}[1]{\textcolor{Red}{[#1]}}       % Red note
\newcommand{\pnote}[1]{\textcolor{Purple}{[#1]}}    % Purple note
\newcommand{\bnote}[1]{\textcolor{Blue}{#1}}        % Blue text
\newcommand{\Max}[1]{\pnote{#1}}                    % Alias for purple note

% Emphasized text
\newcommand{\demph}[1]{\textcolor{RoyalBlue}{\textbf{\slshape #1}}} % Slanted RoyalBlue text


% Claim numbering (the counter restarts after each proof environment)
\newcounter{claimcount}
\setcounter{claimcount}{0}
\newenvironment{claim}{\refstepcounter{claimcount}\par\addvspace{\medskipamount}\noindent\textbf{Claim \arabic{claimcount}:}}{}
\usepackage{etoolbox}
\AtBeginEnvironment{proof}{\setcounter{claimcount}{0}}
\newenvironment{claimproof}{\par\addvspace{\medskipamount}\noindent\textit{Proof of Claim  \arabic{claimcount}.}}{\hfill\ensuremath{\qedsymbol} \tiny{Claim}

  \medskip}
% Add claim support to cleverref
\crefname{claimcount}{Claim}{Claims}

\usepackage[most]{tcolorbox} % for boxed text,
                             % use \begin{tcolorbox[width=0.7\linewidth,
                             % colback=white, colframe=black]}

\usepackage{empheq} % for boxed equations
\usepackage[most]{tcolorbox}
\newtcbox{\mymath}[1][]{%
  nobeforeafter, math upper, tcbox raise base,
  enhanced, colframe=blue!30!black,
  colback=blue!30, boxrule=1pt,
  #1}
\newtcbox{\boxmath}[1][]{%
  nobeforeafter, math upper, tcbox raise base,
  enhanced, colframe=blue!30!black,
  boxrule=1pt,
  #1}


\newenvironment{augmentedmatrix}[1] % environment for making augmented matrix
{\left[\begin{array}{#1}}
    {\end{array}\right]}



% Math Environments
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}

% Redefine the Example environment to include "End of example [number]"
\makeatletter
\let\oldexample\example
\renewenvironment{example}
{\begin{oldexample}}
  {\par\smallskip\hfill   End of Example~\theexample. $\square$    \par\end{oldexample}}
\makeatother

% Custom Math Commands
\newcommand{\vt}{\vspace{5mm}}                       % Vertical space
\newcommand{\fl}[1]{\noindent\textbf{#1}}            % Bold first line
\newcommand{\Fl}[1]{\vspace{5mm}\noindent\textbf{#1}}% Bold first line with space above
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}   % Norm

% Common math symbols
\newcommand{\R}{\mathbb{R}}           % Real numbers
\newcommand{\N}{\mathbb{N}}           % Natural numbers
\newcommand{\C}{\mathbb{C}}           % Complex numbers
\newcommand{\Z}{\mathbb{Z}}           % Integers
\newcommand{\Q}{\mathbb{Q}}           % Rational numbers
\newcommand{\E}{\mathbb{E}}           % Expectation
\renewcommand{\P}{\mathbb{P}}         % Probability (renamed to avoid \P clash)
\newcommand{\indep}{\perp\!\!\!\perp} % Independence symbol

% Operators
\newcommand{\Var}{\mathrm{Var}}   % Variance
\newcommand{\tr}{\mathrm{tr}}     % Trace
\newcommand{\dist}{\mathrm{dist}} % Distance
\newcommand{\diag}{\mathrm{diag}} % Distance


\usepackage{biblatex}
\addbibresource{refs.bib}

\title{Lecture Notes for Math 307:\\Linear Algebra and Differential Equations}
\author{Instructor: Max Hill (Fall 2025)}
\date{Last updated: \today}
\begin{document}

\maketitle
\tableofcontents

\section*{About this document}

These lecture notes were prepared by Max Hill for a 16-week linear algebra course  (MATH
307) at University of Hawaii at Manoa in Fall 2025.

The textbook used is \textit{Linear Algebra and Differential Equations} (2002)
by G.~Peterson S.~Sochacki, in which we cover primarily Chapters 1,2,5, and 6


\newpage
\setcounter{section}{-1}
\section{Tentative Course Outline}

\begin{itemize}
  \item \textbf{Weeks 1-3: Matrices and determinants.} \textit{(Systems of linear
  equations, matrices, matrix operations, inverse matrices, special matrices
  and their properties, and determinants.)}
  \begin{itemize} 
    \item Section 1.1: Systems of Linear Equations
    \item Section 1.2: Matrices and Matrix Operations
    \item Section 1.3: Inverses of Matrices
    \item Section 1.4: Special Matrices and Additional Properties of Matrices
    \item Section 1.5: Determinants
    \item Section 1.6: Further Properties of Determinants
    \item Section 1.7: Proofs of Theorems on Determinants
  \end{itemize}

  \item \textbf{Weeks 4-6: Vector spaces.} \textit{(Vector spaces, subspaces, spanning
  sets, linear independence, bases, dimension, null space, row and column
  spaces, Wronskian.)}
  \begin{itemize}
    \item Section 2.1: Vector Spaces
    \item Section 2.2: Subspaces and Spanning Sets
    \item Section 2.3: Linear Independence and Bases
    \item Section 2.4: Dimension; Nullspace, Rowspace, and Column Space
    \item Section 2.5: Wronskians
  \end{itemize}
  \item \textbf{Weeks 7-11: Linear transformations, spectral theory.} \textit{(Linear
  transformation, eigenvalues and eigenvectors, algebra of linear
  transformations, matrices for linear transformations, eigenvalues and
  eigenvectors, similar matrices, diagonalization, Jordan normal form.)}
  \begin{itemize}
    \item Section 5.1: Linear Transformations
    \item Section 5.2: The Algebra of Linear Transformations
    \item Section 5.3: Matrices for Linear Transformations
    \item Section 5.4: Eigenvalues and Eigenvectors of Matrices
    \item Section 5.5: Similar Matrices, Diagonalization, and Jordan Canonical Form
    \item Section 5.6: Eigenvectors and Eigenvalues of Linear Transformations
  \end{itemize}
  \item \textbf{Midterm Exam}
  \item \textbf{Weeks 12-14: Systems of differential equations.} \textit{(Theory of
  systems of linear differential equations, homogeneous systems with constant
  coefficients, the diagonalizable case, nondiagonalizable case,
  nonhomogeneous linear systems, applications to $2\times 2$ and $3\times 3$
  systems of nonlinear differential equations.)}
  \begin{itemize}
    \item Section 6.1: The Theory of Systems of Linear Differential Equations
    \item Section 6.2: Homogenous Systems with Constant Coefficients: The
    Diagonalizable Case
    \item Section 6.3: Homogenous Systems with Constant Coefficients: The
    Nondiagonalizable Case
    \item Section 6.4: Nonhomogeneous Linear Systems
    \item Section 6.6: Applications Involving Systems of Linear Differential Equations
    \item Section 6.7: $2\times 2$ Systems of Nonlinear Differential Equations
  \end{itemize}
  \item \textbf{Weeks 14-16: Other stuff if time allows.} \textit{(Converting differential
  equations to first order systems (section 6.5), linearization of $2 \times 2$ nonlinear
  systems (???), stability and instability (section 6.7), predator-prey
  equations (section 6.7.1).)}
  \item \textbf{Final Exam}
\end{itemize}

\newpage
\section{2025-08-25 | Week 01 | Lecture 01}
\textit{This lecture is based on textbook section 1.1. Introduction to Systems
  of Linear Equations}

\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The nexus question of this lecture:} What is a system of
      linear equations, and what does it mean to `solve' a system of linear
      equations?}
  \end{tcolorbox}
\end{center}

\subsection{A first example of a system of linear equations}
We begin with something concrete.


\begin{example}[A first example of a \textit{system of linear equations}]
  \label{ex:a-first-example-linear-system-equations}
  Consider the following word problem:

  \begin{quote}
    \textit{A boat travels between two ports on a river 48 miles apart. When traveling
      downstream (i.e., with the current), the trip takes 4 hours, but when
      traveling upstream (i.e., fighting the current), the trip takes 6 hours.}

    \textit{Assume that the boat and the current are both moving at a
      constant speed. What is the speed of the boat in still water, and what is
      the speed of the current?}
  \end{quote}
  This problem is hard to reason through without writing something down, but
  becomes much simpler when we formalize it mathematically with equations. The
  unknowns are (1) \textbf{the speed of the boat in still water} and (2)
  \textbf{the speed of the current}. So let
  \begin{align*}
    x &:= \text{(the speed of the boat in still water)}\\
    y &:= \text{(the speed of the current)}.
  \end{align*}
  The speed of the boat going downstream is $x+y$. Therefore, since
  $(\text{speed})\times (\text{time})= (\text{distance travelled})$, we have
  \begin{equation*}
    4(x+y)=48, \quad \text{or equivalently} \quad  x+y = 12.
  \end{equation*}
  Similarly, the sped of the boat going upstream is $x-y$, so
  \begin{equation*}
    6(x-y)=48, \quad \text{or equivalently} \quad  x-y = 8
  \end{equation*}
  Thus, we have the following \textit{system of linear equations:}
  \begin{equation}\label{eq:first-example-equations}
    \left\{ \begin{array}{l@{}l} x+y=12 \\x-y=8.  \end{array}\right.
  \end{equation}
  This system has \textbf{two equations} and \textbf{two variables} ($x$ and
  $y$).  You have encountered systems of equations like this many times.
  With the help of the technology of algebra, solving this problem (namely,
  solving System \eqref{eq:first-example-equations}) is much easier than solving the
  original word problem.

 
  \begin{itemize}
    \item In this case, the problem can be easily solved
    \textbf{algebraically} using a substitution (e.g., plug $x=8+y$ into the
    first equation and solve for $y$, then solve for $x$ after finding $y$).
    This gives the solution $(x,y) = (10,2)$. The speed of the boat in still
    water is 10mph. The speed of the river current is 2mph.
    \item We can conceive of another type of solution, which uses a
    \textbf{geometric}, rather than algebraic perspective: observe that each
    equation $x+y=12$ and $x-y=8$ represents a line on the $xy$-plane. Plot
    the lines. Their intersection is the point $(10,2)$, which is the
    solution.
    \item However, solving systems of equations like in
    \eqref{eq:first-example-equations} becomes more cumbersome when there are
    lots of variables and equations. Doing substitutions and algebraic
    manipulations will still work, but will be tedius and difficult if you
    have many equations and variables.

    Later, we will introduce a general algorithm which can solve any such
    system. This algorithm is called \demph{Gauss-Jordan elimination}, and it
    will be one of the core techniques that we will use to solve many types of
    problems in this class.
  \end{itemize}
\end{example}


\subsection{Key definitions: linear systems and their solutions}
In this section, we formalize the mathematical objects we are studying.

\begin{definition}[Linear equation]
  \label{def:linear-equation}
  A \demph{linear equation} in the variables $x_{1},\ldots,x_{n}$ is an
  equation that can be written in the form
  \begin{equation*}
    a_{1}x_{1}+a_{2}x_{2}+\ldots+a_{n}x_{n} = b,
  \end{equation*}
  where $a_{1},\ldots,a_{n}$ and $b$ are constants (e.g., fixed real numbers).
  The numbers $a_{1},\ldots,a_{n}$ are called \demph{coefficients}. 
\end{definition}

Note that the variables $x_{1},\ldots,x_{n}$ are not raised to any powers.
That's what makes the equation \textit{linear}. If we had squares or cubes of
some of the $x_{i}$'s, or products like $x_{1}x_{3}$, then the equation would
be quadratic or cubic, or something else, but not linear.

\begin{example}[Examples of linear equations]
  \label{ex:examples-linear-equations-first-examples}
  \begin{itemize}
    \item[]
    \item The equation
    \begin{equation*}
      2x-3y =1
    \end{equation*}
    is a linear equation in the variables $x$ and $y$. Its graph is a line on the
    $xy$-plane.
    \item The equation
    \begin{equation*}
      3x-y+2z = 9
    \end{equation*}
    is a linear equation in the variables $x,y$ and $z$. Its graph is a plane
    in 3-dimensional space (denoted $\R^{3}$).
    \item The equation
    \begin{equation*}
      -x_{1} +5 x_{2}+ \pi^{2} x_{3}+ \sqrt{2}x_{4} = e^{2}
    \end{equation*}
    is a linear equation in the variables $x_{1},x_{2},x_{3},$ and $x_{4}$.
    The coefficients are
    \begin{equation*}
      a_{1} = -1, \quad a_{2} = 5, \quad a_{3} = \pi, \quad \text{and} \quad a_{4}=\sqrt{2}.
    \end{equation*}
    The graph of this linear system is a 3-dimensional hyperplane in 4d-space
    (i.e., $\R^{4}$).

    \fl{Observation:} In these examples, we observe a simple relationship
    between the number of variables and the dimension of the graph:
    \begin{equation*}
      \text{(dimension of graph)} = (\text{\# of variables}) - 1.
    \end{equation*}
    Here, the term \demph{dimension} refers to the number of free variables.
    In the first equation (which is $2x-3y=1$), it's easy to see that if we
    know one of the variables, then the other one is automatically determined.
    So it makes sense that the graph of this equation is of dimension 1 (which
    it is, because it's a line). For the second equation, if we know any $2$
    of the variables, then the third variable is automatically determined, so
    it makes sense that the dimension of the graph is $2$ (which it is,
    because planes are 2-dimensional). Etc.
  \end{itemize}

\end{example}
\begin{definition}[Linear system, solution of a linear system]
  When considered together, a collection of $m$ linear equations
  \begin{equation}\label{eq:defn-system-of-linear-equations}
    \left\{ \begin{array}{l@{}l}
        \ a_{11}x_{1}+a_{12}x_{2}+\ldots+a_{1n}x_{n} = b_{1} \\
        \ a_{21}x_{1}+a_{21}x_{2}+\ldots+a_{2n}x_{n} = b_{2} \\
        \hfill\vdots\hfill \\
        a_{m1}x_{1}+a_{m2}x_{2}+\ldots+a_{mn}x_{n} = b_{m}
      \end{array}\right.
  \end{equation}
  is called a \demph{system of linear equations}, or \demph{linear system} for
  short. A \demph{solution} to a system of linear equations is a set of values
  for $x_{1},\ldots,x_{n}$ which satisfy all equations in
  system \eqref{eq:defn-system-of-linear-equations}.
\end{definition}

\begin{example}[A system of linear equations]
  \label{ex:system-linear-equations-first-example}
  An example of a system of linear equations is
  \begin{equation*}
    \left\{ \begin{array}{l@{}l}
        \hphantom{-2}x\; -\;  \hphantom{3}y \;+\;  \hphantom{4}z =\hphantom{-}0 \\
        \hphantom{-}2x\;-\; 3y\;+\; 4z= -2 \\
        -2x\;-\; \hphantom{3}y\;+\; \hphantom{4}z=\hphantom{-}7
      \end{array}\right.
  \end{equation*}
  When a linear system like this walks in the door, we always first ask two
  basic questions: (1) `how many equations does it have?' and (2) `how many
  variables does it have?'. In this case, we have $m=3$ equations and $n=3$
  variables.
\end{example}
\subsection{How to understand solutions of linear systems geometrically}
Here is a very useful geometric perspective. In
system \eqref{eq:defn-system-of-linear-equations}, we have a system of $m$ equations
expressed in $n$ variables $x_{1},\ldots,x_{n}$. Each of the $m$ equations is
the equation of some hyperplane\footnote{Note: Hyperplanes will be defined
  more formally later, but for now can be thought of as generalized lines or
  planes, since a 1-dimensional hyperplanes is a \textit{line} and a
  2-dimensional hyperplane is a \textit{plane}.} which lives in
$n$-dimensional space ($\R^{n}$). \textit{The solution to the linear system is the
  intersection of these hyperplanes.}

For example, in \cref{ex:system-linear-equations-first-example}, the
`hyperplanes' were lines, and their intersection was the point $(x,y)=(10,2)$.


We will spend a lot of time understanding what hyperplanes look like, and what
intersections of hyperplanes look like.



\newpage
\section{2025-08-27 | Week 01 | Lecture 02}

\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The nexus question of this lecture:} What do solutions to
      linear systems look like?}
      % \vspace{.5em}
      % \begin{enumerate}
      %   \item How do we understand solutions of linear systems geometrically?
      %   \item How can we find a solution to a linear system algebraically,
      %   without resorting to substitution?
      % \end{enumerate}}
  \end{tcolorbox}
\end{center}
\subsection{How to understand solutions of linear systems geometrically}
Here is a very useful geometric perspective. In
system \eqref{eq:defn-system-of-linear-equations}, we have a system of $m$ equations
expressed in $n$ variables $x_{1},\ldots,x_{n}$. Each of the $m$ equations is
the equation of some hyperplane\footnote{Note: Hyperplanes will be defined
  more formally later, but for now can be thought of as generalized lines or
  planes, since a 1-dimensional hyperplanes is a \textit{line} and a
  2-dimensional hyperplane is a \textit{plane}.} which lives in
$n$-dimensional space ($\R^{n}$). \textit{The solution to the linear system is the
intersection of these hyperplanes.}

The clearest example of this can be seen in the linear system:

\begin{example}[The case with two variables]
  \label{ex:case-with-two-variables}
  \begin{equation}\label{eq:2x2-geometric-intuition-example}
    \left\{ \begin{array}{l@{}l}
        a_{11}x + a_{12}y = b_{1}\\
        a_{21}x + a_{22}y = b_{2}\\
      \end{array}\right.
  \end{equation}
  where $a_{12},a_{22}\neq 0$. (In this case, the ``hyperplanes'' are simply
  lines.) Here, the solutions to the first equation are the points on the line
  \begin{equation}\label{eq:line-1}
    y = -\frac{a_{11}}{a_{12}}x + \frac{b_{1}}{a_{12}}
  \end{equation}
  Similarly, the solutions to the second equation are the points on the line
  \begin{equation}\label{eq:line-2}
    y = - \frac{a_{21}}{a_{22}}x + \frac{b_{2}}{a_{22}}.
  \end{equation}
  There are three possible things that can happen when we intersect the two
  lines in \cref{eq:line-1,eq:line-2}:
  \begin{itemize}
    \item \textbf{Case 1.} The two line equations \cref{eq:line-1,eq:line-2}
    represent distinct lines and are not parallel. In this case, their
    intersection consists of a unique point, like this:
    \begin{center}
      \includegraphics[scale=.12]{images/intersecting-lines} % wikipeida
    \end{center}
    In this case, the system \eqref{eq:2x2-geometric-intuition-example} has
    \textbf{exactly one solution}---namely, the intersection of the two lines,
    just like we saw in the boat example.
  
    \item \textbf{Case 2.} The two line equations \cref{eq:line-1,eq:line-2}
    represent two parallel but different lines. In this case, the two lines
    never intersect each other (i.e., there is no point that lies on both
    lines), so the system \eqref{eq:2x2-geometric-intuition-example} has
    \textbf{no solutions}.
  
    \item \textbf{Case 3.} The two equations of lines are the same, so they
    represent the same line. Therefore the intersection of the two lines is
    the entire line. Therefore, there are \textbf{infinitely many solutions}
    to the linear system \eqref{eq:2x2-geometric-intuition-example}. Namely,
    any point $(x,y)$ on the line is a solution to the linear system.
  \end{itemize}
\end{example}
These three cases desribed in \cref{ex:case-with-two-variables} constitute the
following trichotomy:

\begin{theorem}
  A system of linear equations either has (1) exactly one
  solution, (2) no solution, or (3) infinitely many solutions.
\end{theorem}

We haven't proven this fact, only illustrated it for systems of linear
equations like \eqref{eq:2x2-geometric-intuition-example} that have 2
equations and 2 variables. In fact, as we shall see, this fact always holds
for all linear systems of the form given in
\eqref{eq:defn-system-of-linear-equations}, no matter how many equations and
variables.

\subsection{The planar case}
Recall that, geometrically, a line is determined by two features:
\begin{enumerate}
  \item A slope $m$ which determines the direction of the line
  \item A point $(x_{0},y_{0})$ which the line passes through, as this
  determines where the line lives on the $xy$-plane
\end{enumerate}
It is easy to see that these two things determine everything about a line
because the equation of a line can be expressed as
\begin{equation*}
  y-y_{0} = m(x-x_{0})
\end{equation*}
and to write this down, all we need are $m$ and $(x_{0},y_{0}).$

Just like a line, a plane is determined by two things:
\begin{enumerate}
  \item A normal vector $n= \left\langle A,B,C \right\rangle$ which
  determines the tilt of the plane. (Here, $A,B,$ and $C$ are fixed constants)
  \item A point $(x_{0},y_{0},z_{0})$ which the plane passes through, as this
  determines where in 3-d space $(\R^{3})$ the plane lives.
\end{enumerate}
To be precise, a plane $\P$ consists of the set of points $(x,y,z)$ satisfying
the following equation:
\begin{equation}\label{eq:definition-normal-form-eq-of-plane}
  A(x-x_{0}) + B(y-y_{0})+C(z-z_{0})=0
\end{equation}
This is the standard form equation of a plane, and we can write it down if we
know both $n=\left\langle A,B,C \right\rangle $ and $(x_{0},y_{0},z_{0})$. So
if we know those two things, then we know the equation of the plane, meaning
we know everything about it.

By a little bit of algebra, we can rewrite
\cref{eq:definition-normal-form-eq-of-plane} as
\begin{equation*}
  Ax+By+Cz=D
\end{equation*}
where $D = Ax_{0}+By_{0}+Cz_{0}$. This is a linear equation. Just like how the
solutions to a linear equation with 2 variables form a line, the solutions to
a linear equation with 3 variables form a plane.
\begin{example}[A system with three variables]
  \label{ex:system-with-three-variables}
  Suppose we wish to solve the linear system
  \begin{equation*}
    \left\{
      \begin{alignedat}{4}
        &x &{}\;-\; y &{}\;+\; &z{}&= &0 & \\
        2&x&{}\;-\; 3y&{}+\; 4&z{}&= -&2 & \\
        -2&x&{}\;-\; y&{}\;+\; &z {}&= &7& 
      \end{alignedat}
    \right.
  \end{equation*}
  In this case, each equation is the equation of a plane. The planes for the
  first two equations are the following:
  \begin{center}
    \includegraphics[scale=.2]{images/two-planes}
  \end{center}
  The plane for the first equation is in red. The plane for the second
  equation is blue. Any point on the red plane is a solution to the first
  equation $x-y+z=0$. Any point on the blue plane is a solution to the second
  equation $2x-3y+4z=-2$. The two planes interesect in a line. If I pick any
  point on this line, then it satisfies both equations.

  But our system has three equations, so we have a third plane, and the
  intersection of all three planes is a point, as shown:
  \begin{center}
    \includegraphics[scale=.2]{images/three-planes-intersection}
  \end{center}
  In this case, the system of equations has a unique solution, which is the
  unique point of intersection of the planes. Here's the desmos link to see
  the plots of these three planes, if you want to play around with them:
  \begin{center}
    \url{https://www.desmos.com/3d/gpgtw2rjaf}
  \end{center}

  Of course there are other ways that three planes could have intersected. For
  example, two of the planes might be parallel, like the following picture, in
  which case the system will have no solutions:
  \begin{center}
    \includegraphics[scale=.18]{images/two-parallel-planes}
  \end{center}

  Or the three planes could intersect in a line, like the following picture,
  in which case there are infinitely many solutions (credit Noah C. for the
  observation and picture):
  \begin{center}
    \includegraphics[scale=.18]{images/three-planes-intersect-in-a-line}
  \end{center}
  

  
  There are other ways that three planes could intersect as well, but the
  trichotomy stated earlier always holds: their intersection either constists
  of (1) exactly one point, (2) infinitely many points, or (3) zero points.
\end{example}

We've seen in this lecture that for systems of linear equations with two
variables, the solutions are the intersection of lines. For systems of linear
equations with three variables, the solutions are the intersections of planes.
... And for systems with $n>3$ variables, the solutions are the intersections
of hyperplanes.

\newpage

\section{2025-08-29 | Week 01 | Lecture 03}
\textit{This lecture is based on section 1.1 in the textbook}

\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The nexus question of this lecture:} How can
      we solve a linear system without resorting to substitution?}
  \end{tcolorbox}
\end{center}

Recall in the boat example (\cref{ex:a-first-example-linear-system-equations}),
we had the system
\begin{equation*}
  \left\{ \begin{array}{l@{}l} x+y=12 \\ x-y=8 \end{array}\right.
\end{equation*}
And we could solve this using subsitution. Another thing we could have done,
would be to add the second equation to the first, giving us a new, simpler but
equivalent system:
\begin{equation*}
  \left\{ \begin{array}{l@{}l} 2x=20 \\ x-y=8 \end{array}\right.
\end{equation*}
Then divide the firs equation by two
\begin{equation*}
  \left\{ \begin{array}{l@{}l} x=10 \\ x-y=8 \end{array}\right.
\end{equation*}
Then subtract the first equation from the second:
\begin{equation*}
  \left\{ \begin{array}{l@{}l} x=10 \\ -y=-2 \end{array}\right.
\end{equation*}
Then multiply the second equation by $-1$
\begin{equation*}
  \left\{ \begin{array}{l@{}l} x=10 \\ y=2 \end{array}\right.
\end{equation*}
And tada! We have found our solution without doing substitution. But this
example was very simple, so maybe it's special and we can't always do this
sort of thing? Actually, we can. In the rest of the lecture, I'll try to
formalize these sorts of steps we used here and apply them to a more
complicated problem.

The reason I'm doing this is because, in the next lecture, I will begin to
present \demph{Gauss-Jordan elimination} (aka \demph{row reduction}), a
general method which can be used to find the solutions of any system of linear
equation which does not use substitution. For now, the we will work out an
example which motivates the main ideas that will be used by Gauss-Jordan
elimination.

\subsection{Solving a linear system using via simplifying transformations}

\begin{example}[Solving a linear system with elementary operations]
  \label{ex:solving-linear-system-with-elementary-operations}
  Suppose we wish to solve the following system:
  \begin{equation}
    \label{eq:elementary-operation-elimination-example}
    \left\{
      \begin{alignedat}{4}
        &x &{}\;-\; y &{}\;+\; &z{}&= &0 & \quad(E_{1}) \\ 
        2&x&{}\;-\; 3y&{}+\; 4&z{}&= -&2 & \quad(E_{2})\\
        -2&x&{}\;-\; y&{}\;+\; &z {}&= &7& \quad(E_{3})
      \end{alignedat}
    \right.
  \end{equation}
  This system has 3 equations, labeled $E_{1},E_{2},E_{3}$, and 3 variables
  $x,y$ and $z$. Suppose that we know ahead of time that this system has a
  unique solution (we showed this graphically in
  \cref{ex:system-with-three-variables}). Then, in principle, we could solve
  this using substitution, but that would suck. Instead, I will illustrate an
  approach in which we iteratively transform this linear system into
  successively simpler systems until we get to a point where the solution is
  obvious.

  To do this, we will play a game where there are three `moves' available to
  us. The three moves are:
  \begin{enumerate}
    \item Interchange two equations in the system.
    \item Multiply an equation by a nonzero number.
    \item Replace an equation by itself plus a multiple of another equation.
  \end{enumerate}
  These moves are called \demph{elementary operations}, and if we use them
  intelligently, they will allow us to transform the linear system into a
  simpler system.

  Two systems of equations are said to be \demph{equivalent} if they have the
  same solutions. Applying elementary operations always results in an
  equivalent system. Our goal will be to use some combination of elementary
  operations to produce a system of the form
  \begin{equation*}
    \left\{
      \begin{array}{l@{}l}
        x   &= *\\
        y &= *\\
        z &= *\\
      \end{array}\right.
  \end{equation*}
  where each $*$ is a constant which we will have computed. This will be our
  solution to the linear system \eqref{eq:elementary-operation-elimination-example},
  because the two systems will be equivalent.

  First, let's apply operation 3: specifically, by replacing $E_{2}$ with
  $E_{2} - 2E_{1}$:

\begin{equation*}
  \left\{
    \begin{alignedat}{4}
      &x &{}\;-\; y &{}\;+\; &z{}&= &0 \\
      & &{}\;-\; y&{}+\; 2&z{}&= -&2 \\
      -2&x&{}\;-\; y&{}\;+\; &z {}&= &7
    \end{alignedat}
  \right.
\end{equation*}
We have eliminated the $x$ from the second equation, yielding a simpler
system. Let's keep doing this. To eliminate $x$ from equation 3, let's apply
operation 3 again: This time, replace $E_{3}$ with $E_{3}+2 E_{1}$:
\begin{equation*}
  \left\{
    \begin{alignedat}{4}
      &x &{}\;-\; y &{}\;+\; &z{}&= &0 \\
      & &{}\;-\; y&{}+\; 2&z{}&= -&2 \\
      & &{}\; \; -3y&{}\;+\; 3&z {}&= &7
    \end{alignedat}
  \right.
\end{equation*}

Apply operation 3, replace $E_{1}$ with $E_{1}-E_{2}$. This will allow us to
eliminate $y$ from $E_{1}$:

\begin{equation*}
  \left\{
    \begin{alignedat}{4}
      &x &{}\;\;  &{}\;\; -&z{}&= &2 \\
      & &{}\;-\; y&{}+\; 2&z{}&= -&2 \\
      & &{}\; \; -3y&{}\;+\; 3&z {}&= &7
    \end{alignedat}
  \right.
\end{equation*}

Apply operation 3, replace $E_{3}$ with $E_{3}-3E_{2}$. This will allow us to
eliminate $y$ from $E_{3}$:
\begin{equation*}
  \left\{
    \begin{alignedat}{4}
      &x &{}\;\;  &{}\;\; -&z{}&= &2 \\
      & &{}\;-\; y&{}+\; 2&z{}&= -&2 \\
      & &{}\; \; &{}\;-\; 3&z {}&= &13
    \end{alignedat}
  \right.
\end{equation*}
Apply operation 2 twice: multiply both the first and second equations by $3$:
\begin{equation*}
  \left\{
    \begin{alignedat}{3}
      3x&   &{}\;\; -3&z{}= 6 \\
        & {}\;-\; 3y&{}\;+6&z{}= -6 \\
        & {}\; \; &{}\;\;- 3&z = 13
    \end{alignedat}
  \right.
\end{equation*}
Apply operation 3, twice. First, replace $E_{1}$ with $E_{1}-E_{3}$. Then
replace $E_{2}$ with $E_{2}+2E_{3}$. Doing both of these, we get:
\begin{equation*}
  \left\{
    \begin{alignedat}{3}
      3x \quad& &  & =&{} -7 \\
              & &-3y \quad \quad& =&{} 20 \\
              & & -3z& =&{} 13
    \end{alignedat}
  \right.
\end{equation*}
Apply operation $2$ by multiplying the first equation by $1/3$. Then multiply
the second and third equations both by $-1/3$:
\begin{equation*}
  \left\{
    \begin{alignedat}{3}
      x \quad& &  & =&{} -7/3 \\
             & &y \quad \quad& =&{} -20/3 \\
             & &z& =&{} -13/3
    \end{alignedat}
  \right.
\end{equation*}
This is the solution to the original equation. We have used elementary
operatons to reduce our original linear system
\cref{eq:elementary-operation-elimination-example} to the above system, which
equivalent to the original system.

While solving this system was still a lot of (tedious) work, it was still
probably simpler than doing substitution.
\end{example}

\subsection{Representing a linear system as an augmented matrix}
In the procedure presented in
\cref{ex:solving-linear-system-with-elementary-operations}, we didn't really
need to track the variables, only the \textit{coefficients} and the
\textit{quantities on the right hand sides} of the equations. Instead of
working with the equations directly, it will be simpler to work with the
following matrix, called the \demph{augmented matrix} corresponding go
\cref{eq:elementary-operation-elimination-example}:
\begin{equation*}
  \begin{augmentedmatrix}{ccc|c}
    1& -1& 1& 0 \\
    2& -3& 4& -2 \\
    -2& -1& 1& 7
  \end{augmentedmatrix}.
\end{equation*}
Comparing this with system \eqref{eq:elementary-operation-elimination-example}, it becomes
clear that the augmented matrix was obtained essentially by just erasing the
variables $x,y,$ and $z$ in \eqref{eq:elementary-operation-elimination-example}, and
then placing what remains into an array. We also drew a vertical line to the
separate the left- and right-hand sides of the equations. Inside the augmented
matrix, the $3\times 3$ submatrix  of coefficients
\begin{equation*}
  \begin{bmatrix}
    1& -1& 1\\
    2& -3& 4\\
    -2& -1& 1
  \end{bmatrix}
\end{equation*}
is called the \demph{coefficient matrix} of the system. 

More precise definitions are as follows:

\begin{definition}[Augmented Matrix]
  Given a linear system of the form \eqref{eq:defn-system-of-linear-equations},
  the \demph{augmented matrix} is
  \begin{equation*}
    \begin{augmentedmatrix}{cccc|c}
      a_{11}&a_{12}&\ldots& a_{1n}& b_{1}\\
      a_{21}&a_{22}&\ldots& a_{2n}& b_{2}\\
      \vdots &\vdots & &\vdots &\vdots \\
      a_{m1}&a_{m2}&\ldots& a_{mn}& b_{m}
    \end{augmentedmatrix}
  \end{equation*}
  and the \demph{coefficient matrix} is the matrix
  \begin{equation*}
    \begin{bmatrix}
      a_{11}&a_{12}&\ldots& a_{1n}\\
      a_{21}&a_{22}&\ldots& a_{2n}\\
      \vdots&\vdots & &\vdots \\
      a_{m1}&a_{m2}&\ldots& a_{mn}
    \end{bmatrix}.
  \end{equation*}
\end{definition}

This is our first application of 
\newpage
\section{2025-09-03 | Week 02 | Lecture 04}

\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The nexus question of this lecture:} What is Gauss-Jordan
      elimination (aka: row reduction) and how do we use it to solve linear systems?}
  \end{tcolorbox}
\end{center}

Now I will present \demph{Gauss-Jordan elimination}. This is also called
\demph{Gaussian elimination}, or more commonly, \demph{row reduction}. I will
illustrate it by means of an example.

\subsection{Using row reduction to solve a linear system with a unique solution}
Suppose we wish to solve
\begin{equation}\label{eq:Gauss-Jordan-elimination-example}
  \left\{
    \begin{alignedat}{4}
      &x &{}\;-\; y &{}\;+\; &z{}&= &0 \\
      & &{}\;-\; y&{}+\; 2&z{}&= -&2 \\
      -2&x&{}\;-\; y&{}\;+\; &z {}&= &7
    \end{alignedat}
  \right.
\end{equation}

\Fl{Steps:} We initialize the algorithm by setting up an \demph{augmented
  matrix} corresponding to the system. For the system in
\eqref{eq:Gauss-Jordan-elimination-example}, the augmented matrix is
\begin{equation*}
  \begin{augmentedmatrix}{ccc|c}
    1& -1& 1& 0 \\
    2& -3& 4& -2 \\
    -2& -1& 1& 7
  \end{augmentedmatrix}.
\end{equation*}
\begin{itemize}
  \item The matrix to the left of the vertical row is the \demph{coefficient matrix.}
  \item A line of numbers going from left to right is called a \demph{row} of
  the matrix. A line of numbers going down the matrix is a \demph{column.}
\end{itemize}
Gauss-Jordan elimination is like a game where the player has three possible
moves, called \demph{row operations}.
\begin{enumerate}
  \item Interchange two rows.
  \item Multiply a row by a nonzero number.
  \item Replace a row by itself plus a multiple of another row.
\end{enumerate}
The player does row operations with the \textbf{goal} of making the diagonal
entries of the coefficient matrix 1's and making as many of the other numbers
zero, if possible. Here are the row operations for this example:
\begin{equation*}
  \phantom{ \overset{R_{2}-2R_{1}\to R_{2}}{\longrightarrow}}
  \begin{augmentedmatrix}{ccc|c}
    1& -1& 1& 0 \\
    2& -3& 4& -2 \\
    -2& -1& 1& 7
  \end{augmentedmatrix}
  \overset{R_{2}-2R_{1}\to R_{2}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    1& -1& 1& 0 \\
    \mathbf{0}& \mathbf{-1}& \mathbf{2}& \mathbf{-2} \\
    -2& -1& 1& 7
  \end{augmentedmatrix}
  \overset{R_{3}+2R_{1} \to R_{3}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    1& -1& 1& 0 \\
    0& -1& 2& -2 \\
    \mathbf{0}& \mathbf{-3}& \mathbf{3}& \mathbf{7}
  \end{augmentedmatrix}
\end{equation*}
\begin{equation*}
  \overset{R_{1}-R_{2} \to R_{1}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    \mathbf{1}& \mathbf{0}& \mathbf{-1}& \mathbf{2} \\
    0& -1& 2& -2 \\
    0& -3& 3& 7
  \end{augmentedmatrix}
  \overset{R_{3}-3R_{2} \to R_{3}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    1& 0& -1& 2 \\
    0& -1& 2& -2 \\
    \mathbf{0}& \mathbf{0}& \mathbf{-3}& \mathbf{13}
  \end{augmentedmatrix}
  \overset{(-1)\cdot R_{2} \to R_{2}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    1& 0& -1& 2 \\
    \mathbf{0}& \mathbf{1}& \mathbf{-2}& \mathbf{2} \\
    0& 0& -3& 13
  \end{augmentedmatrix}
\end{equation*}
\begin{equation*}
  \overset{(-\frac{1}{3})\cdot R_{3} \to R_{3}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    1& 0& -1& 2 \\
    0& 1& -2& 2 \\
    \mathbf{0}& \mathbf{0}& \mathbf{1}& \mathbf{-13/3}
  \end{augmentedmatrix}
  \overset{R_{2}+2R_{3} \to R_{2}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    1& 0& -1& 2 \\
    \mathbf{0}& \mathbf{1}& \mathbf{0}& \mathbf{-20/3} \\
    0& 0& 1& -13/3
  \end{augmentedmatrix}
  \overset{R_{1}+R_{3} \to R_{1}}{\longrightarrow}
  \begin{augmentedmatrix}{ccc|c}
    \mathbf{1}& \mathbf{0}& \mathbf{0}& \mathbf{-7/3} \\
    0& 1& 0& -20/3 \\
    0& 0& 1& -13/3
  \end{augmentedmatrix}
\end{equation*}
We now convert the augmented matrix back to a system of linear equations:
\begin{equation*}
  \left\{
    \begin{alignedat}{4}
      &1x &{}\;-\; 0y &{}\;+\; &0z{}&= -&7/3 \\
      &0x&{}\;-\; 1y&{}\;+\; &0z{}&= -&20/3 \\
      &0x&{}\;-\; 0y&{}\;+\; &1z {}&= -&13/3
    \end{alignedat}
  \right.
\end{equation*}
or more simply,
\begin{align*}
  x &= -7/3\\
  y &= -20/3\\
  z &= -13/3
\end{align*}
We can check that this is a solution to the original system of equations
\eqref{eq:Gauss-Jordan-elimination-example}.

\subsection{The goal when doing row reduction}
In the previoux example, we used row reduction Gauss-Jordan elimination to
solve a linear system. The example we did had a unique solution. When that
happens we can reduce the coefficient matrix to a matrix like
\begin{equation*}
  \begin{bmatrix}
    1&0&0\\
    0&1&0\\
    0&0&1\\
  \end{bmatrix}
\end{equation*}
possibly with one or more rows of zeros at the bottom. (It may be larger or
smaller depending on the number of equations and variables).

But in general, as we've seen, a linear system either has (1) one solution,
(2) no solutions, or (3) infinitely many solutions. And if cases (2) or (3)
happen, then we won't be able to do that. So we need to relax our ``goal''
when doing row reduction.

\textbf{Our new goal} is to reduce the coefficient matrix to \demph{reduced
  row-echelon form}, which in the case of a linear system with three equations
and three variables, means it should look like one of these
\begin{equation*}
  \begin{bmatrix}
    1&0&0\\
    0&1&0\\
    0&0&1\\
  \end{bmatrix}
  \quad \text{or} \quad
  \begin{bmatrix}
    1&0&\#\\
    0&1&\#\\
    0&0&0\\
  \end{bmatrix}
  \quad \text{or} \quad
  \begin{bmatrix}
    1&\#&\#\\
    0&0&0\\
    0&0&0\\
  \end{bmatrix}
\end{equation*}
where $\#$ denotes any arbitrary number.

\begin{definition}
  More precisely, a coefficient matrix
  \begin{equation*}
    \begin{bmatrix}
      a_{11}&a_{12}&\ldots& a_{1n}\\
      a_{21}&a_{22}&\ldots& a_{2n}\\
      \vdots&\vdots & &\vdots \\
      a_{m1}&a_{m2}&\ldots& a_{mn}
    \end{bmatrix}.
  \end{equation*}
  is said to be in \demph{reduced row echelon form} if
  \begin{enumerate}
    \item Any rows of zeros appear at the bottom
    \item The leftmost nonzero entry of all other rows equals 1 (the ``leading
    1's'')
    \item Each leading 1 of a nonzero row appears to the right of the leading
    row above it
    \item All the other entries of a column containing a leading $1$ are zero
  \end{enumerate}
\end{definition}
This definition is \textit{general}: it applies to any system with $m$ equations and
$n$ variables. The pattern will become natural once you've worked a few
(dozen?) examples.

\subsection{Using row reduction to solve a system with infitely many
solutions}

Here's an example which shows what happens when we try to solve a linear
system with infinitely many solutions:

\begin{example}[Using row reduction to solve a system with infitely many
  solutions]
  \label{ex:using-row-reduction-solve-system-with-infitely-many-solutions}
  We wish to solve the system
  \begin{equation*}
    \left\{ \begin{array}{l@{}l}
      \phantom{-}2x+3y-z\ &=3  \\
      -x-\phantom{2}y+3z\ &=0  \\
      \phantom{-}x+2y+2z\ &=3  \\
      \phantom{2x+3}y+5z\ &=3 
      \end{array}\right. 
  \end{equation*}

  This system has 4 equations and 3 variables. Each equation represents a
  plane. The solutions, if there are any, will be the intersection of these
  four planes. I've plotted the planes in Desmos:

  \begin{center}
    \includegraphics[scale=.2]{images/line-of-solutions}
  \end{center}
  It looks like the four planes intersect in a line. So we should expect
  infinitely many solutions.
  
  \Fl{Step 1.} Write down the augmented matrix of the system.

  \begin{equation*}
    \begin{augmentedmatrix}{ccc|c}
      2&3&-1&3\\
      -1&-1&3&0\\
      1&2&2&3\\
      0&1&5&3
    \end{augmentedmatrix}
  \end{equation*}

  \Fl{Step 2.} Do some combination or row reduction steps until the
  coefficient matrix is in reduced row echelon form. (This is Example 3 in the
  textbook, refer there for the steps.)
  
  \Fl{Step 3.} Our matrix is now in reduced row echelon form:
  \begin{equation*}
    \begin{augmentedmatrix}{ccc|c}
      1&0&-8&-3\\
      0&1&5&3\\
      0&0&0&0\\
      0&0&0&0
    \end{augmentedmatrix}
  \end{equation*}
  Let's convert it back into a linear system of equations:
  \begin{align*}
    x-8z&=-3\\
    y+5z&=3\\
    0&=0\\
    0&=0
  \end{align*}
  Therefore, we have
  \begin{equation}\label{eq:example-of-line-of-solutions}
    \left\{ \begin{array}{l@{}l} x=-3+8z \\ y=3-5z \end{array}\right.
  \end{equation}
  where $z$ is any real number. There are no restrictions on the value of $z$.
  Any choice of $z$ gives us a valid solution to our original system of
  equations. If $z=0$, we have the solution $(x,y,z)=(-3,3,0)$. If $z=1$, then
  we have the solution $(x,y,z)= (5,-2,1)$, and so forth.

  \textbf{Interpretation:} In this case, $z$ is called a \demph{free variable}
  and $x$ and $y$ are called \demph{dependent variables}. The solutions to the
  original linear system consist of all points on a line which cuts through
  3-dimensional space $\R^{3}$. \cref{eq:example-of-line-of-solutions} gives
  us a parametric equation of the line. The set of solutions is 1-dimensional,
  because it is a line.
\end{example}

We've covered two of the three cases. For the last case, we consider the
question \textit{what happens if we attempt to solve a linear system that has
  no solutions?}


\subsection{Using row reduction to attempt to solve a system with no
  solutions}
\begin{example}[Using row reduction to attempt to solve a system with no
  solutions]
  \label{ex:using-row-reduction-attempt-solve-system-with-no-solutions}
  Suppose we wish to solve the system
  \begin{align*}
    2x+y-z&=3\\
    -x-y+2z&=0\\
    -x-y+2z&=4\\
  \end{align*}
  Here's a plot of the planes:
  \begin{center}
    \includegraphics[scale=.2]{images/inconsistent-system}
  \end{center}
  Their intersection if the three planes is \textit{empty}: there is no point
  which lies on all three planes. So this system has no solution. What happens
  when we try to use row reduction? 

  \Fl{Step 1.} Write
  down the augmented matrix:
  \begin{equation*}
    \begin{augmentedmatrix}{ccc|c}
      2&1&-1&3\\
     -1&-1&2&0\\
      -1&-1&2&4
    \end{augmentedmatrix}
  \end{equation*}

  \Fl{Step 2.} Do row reduction to get to reduced row echelon form (I'm
  skipping steps here):
  \begin{equation*}
    \begin{augmentedmatrix}{ccc|c}
      1&0&1&3\\
      1&-1&3&3\\
      0&0&0&4
    \end{augmentedmatrix}
  \end{equation*}

  \Fl{Step 3.} Convert back to a linear system:
  \begin{equation*}
    \left\{ \begin{array}{l@{}l}
        x\phantom{-y3\ }+z&=3\\
        x-y+3z\ &=3\\
        0&=4 \\  \end{array}\right.
  \end{equation*}
  The last equation is never true, no matter what values we choose for $x,y$
  and $z$. So the system has no solution
\end{example}
\subsection{Notes and resources for learning row reduction}

\Fl{NOTE:} I'm not going to do a lot of examples of row reduction because it's
so time consuming that it's not a great use of lecture time. So I expect you
to teach yourself how to do row reduction. If you want to see more examples,
some good online videos are
\begin{center}
  \url{https://www.youtube.com/watch?v=OP2aQUOevhI}
  
  \url{https://www.youtube.com/watch?v=eDb6iugi6Uk}
\end{center}

There are also some nice online tools to help with row reduction, for example:

\begin{center}
  \url{https://textbooks.math.gatech.edu/ila/demos/rrinter.html}

  \url{https://www.math.odu.edu/~bogacki/cgi-bin/lat.cgi?c=roc}
\end{center}

\fl{General advice:}
\begin{enumerate}
  \item Of course I'm going to ask you to row-reduce matrices on your exams,
  and for that you'll need to be able to do it by hand, without a calculator.
  \item Write down all your steps, including a new matrix at every step in an
  organized way.
  \item Use the notation $R_{i}\leftrightarrow R_{j}$ to indicate a swap of
  rows $i$ and $j$; $cR_{i}\to R_{i}$ to indicate a multiplication of row $i$
  by a constant $c$; and, $R_{i}+cR_{j}\to R_{i}$ to indicate that you've
  added $c$ times row $j$ to row $i$.
  \item Work left to right, top to bottom. Start by making the top left entry
  1. Then use it to make all the numbers below it zero. Then go to the second
  column, second row, and make that 1. Then make everything beneath it zero.
  Etc.
\end{enumerate}

\newpage
\section{2025-09-05 | Week 03 | Lecture 05}
\textit{This lecture is based on section 1.2 in the textbook.}

\subsection{Matrices and matrix notation}
\begin{center}
  \begin{tcolorbox}[width=0.9\textwidth, colback=white, colframe=black]
    \textit{\textbf{The nexus question of this lecture:} What is a matrix, and
      what are the fundamental algebraic operations we can do with it?}
  \end{tcolorbox}
\end{center}
A \demph{matrix} is a rectangular array of objects, usually numbers, which are
called \demph{entries}. If the number of rows and the number of columns are
equal, the matrix is said to be a \demph{square matrix}.

For example,
\begin{equation*}
  \begin{bmatrix}
    1&0&3\\
    2&5&-3
  \end{bmatrix}
  \quad \text{or} \quad
  \underbrace{\begin{bmatrix}
    1&0&-7&5
  \end{bmatrix}}_{a \demph{row vector}}
  \quad \text{or} \quad
  \underbrace{\begin{bmatrix}
    1\\4\\0
  \end{bmatrix}}_{a \demph{column vector}}
  \quad \text{or} \quad
  \begin{bmatrix}
    1&0\\
    0&-1
  \end{bmatrix}
\end{equation*}
A matrix with $m$ rows and $n$ columns takes the form
\begin{equation*}
  A=
  \begin{bmatrix}
    a_{11}&a_{12}&\ldots& a_{1n}\\
    a_{21}&a_{22}&\ldots& a_{2n}\\
    \vdots&\vdots & &\vdots \\
    a_{m1}&a_{m2}&\ldots& a_{mn}
  \end{bmatrix}.
\end{equation*}
Such a matrix is said to be an \demph{$m\times n$ matrix}. The pair $(m,n)$
is called the \demph{dimensions} of the matrix (i.e., the number of rows and
number of columns). If $m=n$, then the matrix is said to be a \demph{square matrix}.

The set of all $m\times n$ matrices with real entries is denoted
\begin{equation*}
  M_{m\times n}(\R)
\end{equation*}
For example, in set notation
\begin{equation*}
  M_{2\times 3}(\R) = \left\{
    \begin{bmatrix}
      a_{11}&a_{12}&a_{13}\\
      a_{21}&a_{22}&a_{23}\\
    \end{bmatrix}
    :
    a_{11},a_{12},a_{13},a_{21},a_{22},a_{23}\in \R 
  \right\} 
\end{equation*}

\begin{notation}
  It is common to denote a matrix compactly using the notation
  \begin{equation*}
    A=[a_{ij}]
    \quad \text{or} \quad
    A=(a_{ij})
  \end{equation*}
  To denote the entry at row $i$, column $j$, we write either
  \begin{equation*}
    \text{ent}_{ij}(A)
    \quad \text{or more simply,} \quad
    a_{ij}
  \end{equation*}
  For example, if
  \begin{equation*}
    A = (a_{ij}) = \begin{bmatrix}
      -1&2&1\\
      5&4&-9\\
      3&-4&7
    \end{bmatrix}
  \end{equation*}
  then $a_{23} = \text{ent}_{23}(A) = -9$, and $a_{21}=\text{ent}_{21}=5$.
\end{notation}

Vectors are a special case of matrices. An $n$-dimensonal vector is an
$n\times 1$ matrices (a column matrix, typically).

\subsection{Matrix operations: scaling, addition and multiplication}
We can \demph{multiply matrices by a scalar}, in the obvious way:
$10\times \begin{bmatrix}
  3&4\\
  -2&0
\end{bmatrix}
=
\begin{bmatrix}
  30&40\\
  -20&0
\end{bmatrix}$.

\noindent We can \demph{add matrices}, also in the obvious way:
\begin{equation*}
  \begin{bmatrix}
    3&4\\
    -2&0
  \end{bmatrix}
  +
  \begin{bmatrix}
    1&5\\
    2&10
  \end{bmatrix}
  =
  \begin{bmatrix}
    4&9\\
    0&10
  \end{bmatrix}.
\end{equation*}
But note that we can only add two matrices if they have the same dimensons:
\begin{equation*}
  \begin{bmatrix}
    5&2\\
    -2&-1
  \end{bmatrix}
  +
  \begin{bmatrix}
    -1&10\\
    4&7\\
    2&5
  \end{bmatrix}
  = \text{ undefined}.
\end{equation*}
We can also \textbf{multiply} matrices, but before defining matrix
multiplication, it will be helpful to recall the notion of dot product.
Suppose we have two vectors of the same length:
\begin{equation*}
  X =
  \begin{bmatrix}
    x_{1}&x_{2}&\cdots&x_{n}
  \end{bmatrix}
  \quad \text{and} \quad
  Y =
  \begin{bmatrix}
    y_{1}&y_{2}&\cdots&y_{n}
  \end{bmatrix},
\end{equation*}
then the \demph{dot product} of $X$ and $Y$, denoted $X\cdot Y$, is
\begin{align*}
  X\cdot Y 
  &= x_{1}y_{1}+ x_{2}y_{2}+\ldots+x_{n}y_{n} = \sum_{i=1}^{n}x_{i}y_{i}.
\end{align*}
\demph{Matrix multiplication} is defined in the following way:


\begin{tcolorbox}[colframe=black, colback=gray!5, arc=2mm, boxrule=0.8pt,
  breakable, title=Matrix Multiplication,   coltitle=black,colbacktitle=black!10]
  If $A=(a_{ij})$ is a $p\times n$ matrix and $B=(b_{ij})$ is an $n\times q$
  matrix, then we can can think of $A$ and $B$ as
  \newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
  \newcommand*{\horzbar}{\,\rule[.5ex]{2.5ex}{0.5pt}\,}
  \begin{equation*}
    A =
    \left[\begin{array}{c}
        \horzbar  A_{1}  \horzbar \\
        \horzbar  A_{2}  \horzbar \\
        \vdots             \\
        \horzbar  A_{p} \horzbar
      \end{array}\right]
    \quad \text{and} \quad
    B = 
    \left[
      \begin{array}{cccc}
        \vertbar & \vertbar &        & \vertbar \\
        B_{1}    & B_{2}    & \ldots & B_{q}    \\
        \vertbar & \vertbar &        & \vertbar 
      \end{array}
    \right]
  \end{equation*}
  where $A_{1},\ldots,A_{p}$ are the $1\times n$ row vectors
  \begin{equation*}
    A_{i} =
    \begin{bmatrix}
      a_{i1}& a_{i2}& \cdots & a_{in}
    \end{bmatrix}
    \quad (i=1,\ldots,p)
  \end{equation*}
  and $B_{1},\ldots,B_{q}$ are $n\times 1$ column vectors:
  \begin{equation*}
    B_{j} =
    \begin{bmatrix}
      b_{1j}\\b_{2j}\\\vdots\\b_{nj}
    \end{bmatrix}
    \quad (j=1,2,\ldots,q)
  \end{equation*}
Then the product $P=AB$ is a $p\times q$ matrix $P=(p_{ij})$, whose entries are
\begin{align*}
  p_{ij} 
  &= A_{i}\cdot B_{j} \\
  &= \sum_{k=1}^{n}a_{ik}b_{kj}.
\end{align*}
\end{tcolorbox}
For example,
\begin{equation*}
  \begin{bmatrix}
    1&2\\
    3&4
  \end{bmatrix}
  \begin{bmatrix}
    5&6\\
    7&8
  \end{bmatrix}
  =
  \begin{bmatrix}
    1\cdot 5+2\cdot7 & 1\cdot 6 + 2\cdot8\\
    3\cdot 5+4\cdot7 & 3\cdot 6 + 4\cdot8
  \end{bmatrix}
  =
  \begin{bmatrix}
    19&22\\
    43&50
  \end{bmatrix}
\end{equation*}

\Fl{Dimensionality requirement for matrix multiplication:} we can only multiply
two matrices if their dimensions match in the right way. If $A$ is a
$p\times n$ matrix, and $B$ is an $\tilde{n} \times q$ matrix, then the
product $AB$ is defined if and only if $n=\tilde{n}$. That is,
\begin{equation*}
  AB \text{ is } \left\{ \begin{array}{l@{\quad \text{if }\quad}l} \text{a $p\times q$ matrix} & n=\tilde{n} \\
      \text{undefined}& n\neq \tilde{n} \end{array}\right.
\end{equation*}

\Fl{Properties of matrix multiplication:} Perhaps surprisingly, despite
matrix multiplication's complicated definition, it nonetheless behaves sort of
like regular multiplication in that the \demph{associative property} and
\demph{distributed property} both hold. That is,
\begin{equation*}
  ABC = (AB)C = A(BC) \quad \text{(associative property)}
\end{equation*}
and
\begin{gather*}
  A(B+C) = AB+BC \quad \text{(left distributive property)}\\
  (B+C)A = BA+CA \quad \text{(right distributive property)}
\end{gather*}

It's not obvious why the properties always hold; they require proof. For now,
we will take them as a given.


\subsection{Special classes of matrices }

\subsubsection{Diagonal matrices}
If $A=(a_{ij})$ is a square (i.e., $n\times n$) matrix, then the entries
$a_{11},a_{22},a_{33},\ldots,a_{nn}$ are called the \demph{diagonal
  entries}. A square matrix of the form
\begin{equation*}
  A =\begin{bmatrix}
    a_{11}&0&0&\ldots& 0\\
    0&a_{22}&0&\ldots& 0\\
    0&0&a_{33}&\ldots& 0\\
    \vdots&\vdots & &\ddots \\
    0&0&0&\ldots& a_{nn}
  \end{bmatrix}.
\end{equation*}
is called a \demph{diagonal} matrix, and the notation used for such matrices
is $A=\diag(a_{11},\ldots,a_{nn})$.

\subsubsection{Identity matrix}
An \demph{identity matrix} is a diagonal matrix with 1's on its diagonal (and
0's everywhere else). For example,
\begin{equation*}
  \begin{bmatrix}
    1&0\\
    0&1
  \end{bmatrix}
  \quad \text{or} \quad
  \begin{bmatrix}
    1&0&0\\
    0&1&0\\
    0&0&1
  \end{bmatrix}
  \quad \text{or} \quad
  \begin{bmatrix}
    1&0&0&0\\
    0&1&0&0\\
    0&0&1&0\\
    0&0&0&1\\
  \end{bmatrix}
  \quad \text{et cetera}
\end{equation*}
Identity matrices are always square. The notation for an $n\times n$
identity matrix is $I_{n}$, or more simply $I$.

An identity matrix has the property that when you multiply it by another matrix, it doesn't
change the other matrix. For example,
\begin{equation*}
  \begin{bmatrix}
    5&6\\
    2&7
  \end{bmatrix}
  \begin{bmatrix}
    1&0\\
    0&1
  \end{bmatrix}
  =
  \begin{bmatrix}
    5&6\\
    2&7
  \end{bmatrix}
\end{equation*}
It's like multiplying a number by 1.

\subsubsection{Zero matrices}

A matrix whose entries are all zeros is called a \demph{zero matrix}, like
\begin{equation*}
  \begin{bmatrix}
    0&0\\
    0&0
  \end{bmatrix}
  \quad \text{or} \quad
  \begin{bmatrix}
    0&0&0\\
    0&0&0\\
    0&0&0
  \end{bmatrix}
  \quad \text{or} \quad
  \begin{bmatrix}
    0&0&0\\
    0&0&0\\
  \end{bmatrix}
  \quad \text{et cetera}
\end{equation*}
The book uses the notation $\mathbf{O}_{m\times n}$ to denote an $m\times n$
zero matrix, or sometimes even just $\mathbf{O}$.

\newpage
\section{2025-09-?? | Week 3 | Lecture 06}


On the other hand, matrix multiplication doesn't satisfy the
\demph{commutative property} (which says that, e.g., $5\cdot 3 = 3\cdot 5$)
because the products $AB$ and $BA$ are usually not equal. To be precise, two
matrices $A$ and $B$ are said to \demph{commute} if $AB=BA$, but again, this
usually doesn't happen. An example of two matrices that do commute are
\begin{equation*}
  A =
  \begin{bmatrix}
    1&1\\
    0&1
  \end{bmatrix}
  \quad \text{and} \quad
  B =
  \begin{bmatrix}
    2&3\\
    0&2
  \end{bmatrix}
\end{equation*}
These two matrices commute because
\begin{equation*}
  AB=BA =
  \begin{bmatrix}
    2&5\\
    0&2
  \end{bmatrix}.
\end{equation*}

If two matrices satisfy the property that $AB=BA=I$, then they are said to
be \demph{inverses}. An example are the matrices
\begin{equation*}
  A =
  \begin{bmatrix}
    1&2\\
    3&5
  \end{bmatrix}
  \quad \text{and} \quad
  B =
  \begin{bmatrix}
    -5&2\\
    3&-1
  \end{bmatrix}
\end{equation*}
since $AB=I$ and $BA=I$. In this case, we write $B=A^{-1}$ (which doesn't
mean $1/A$). This is analogous to when we multiply two numbers like
$3\cdot \frac{1}{3}= \frac{1}{3}\cdot 3=1$. One difference, however, is that
for matrices, is possible for $AB=I$ but $BA\neq I$. Another difference is
that while every nonzero real number has an inverse, many matrices do not
have inverses. A matrix that doesn't have an inverse is called
\demph{singular} or \demph{noninvertible}. A matrix that has an inverse, is
called \demph{nonsingular} or \demph{invertible}.


\textbf{Question:} \textit{Why is matrix multiplication defined in such a
  seemingly bizzare way?} A good explanation will have to come later in the
course, but the beginnings of an answer is that it allows us to represent
systems of linear equations with matrix equations.

Indeed, suppose we have any linear system, like this:
\begin{equation*}
  \left\{ \begin{array}{l@{}l}
      \ a_{11}x_{1}+a_{12}x_{2}+\ldots+a_{1n}x_{n} = b_{1} \\
      \ a_{21}x_{1}+a_{21}x_{2}+\ldots+a_{2n}x_{n} = b_{2} \\
      \hfill\vdots\hfill \\
      a_{m1}x_{1}+a_{m2}x_{2}+\ldots+a_{mn}x_{n} = b_{m}
    \end{array}\right.
\end{equation*}
Define the three matrices
\begin{equation*}
  A =
  \begin{bmatrix}
    a_{11}&a_{12}&\ldots& a_{1n}\\
    a_{21}&a_{22}&\ldots& a_{2n}\\
    \vdots&\vdots & &\vdots \\
    a_{m1}&a_{m2}&\ldots& a_{mn}
  \end{bmatrix},
  \quad
  X =
  \begin{bmatrix}
    x_{1}\\x_{2}\\\vdots\\x_{n}
  \end{bmatrix}
  \quad \text{and} \quad
  B =
  \begin{bmatrix}
    b_{1}\\b_{2}\\\vdots\\b_{m}
  \end{bmatrix}
\end{equation*}
Then we can represent the linear system compactly as the matrix equation
\begin{equation*}
  AX=B.
\end{equation*}
If we know $A^{-1}$, then we can multiply both sides of this equation on the
left by
\begin{equation*}
  A^{-1}AX = A^{-1}B
\end{equation*}
which simplifies to
\begin{equation*}
  X = A^{-1}B.
\end{equation*}


\end{document}